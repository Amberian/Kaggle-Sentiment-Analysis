{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "参考链接：https://www.jianshu.com/p/57a9b6103fe5\n",
    "提交链接：https://www.kaggle.com/c/word2vec-nlp-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.lancaster import LancasterStemmer#词干提取\n",
    "from nltk.stem import WordNetLemmatizer#词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('data/labeledTrainData.tsv',header=0,sep='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:3][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本预处理\n",
    "1. 去除HTML标签：BeautifulSoup\n",
    "2. 将所有词转为小写\n",
    "2. 去除数字和标点，用空格置换，（后续可以保留类似于\"!!!\" or \":-(\" 这样的带有情感的符号）\n",
    "3. 去除停用词：nltk.stopwords\n",
    "4. 词干提取和词性还原：https://www.cnblogs.com/cwp-bg/p/9510513.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除HTML标签\n",
    "from bs4 import BeautifulSoup\n",
    "example_1=BeautifulSoup(train_data['review'][0],'lxml')\n",
    "example_1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转化为小写\n",
    "example_2=example_1.get_text().lower()\n",
    "#去除数字和标点\n",
    "import re\n",
    "example_3=re.sub('[^a-zA-Z]',' ',example_2)\n",
    "example_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词，获取token\n",
    "from nltk.tokenize import word_tokenize\n",
    "word=word_tokenize(example_3)\n",
    "print(len(word))\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "stops=stopwords.words('english')\n",
    "word2=[w for w in word if w not in stops]\n",
    "word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词干提取和词形还原\n",
    "from nltk.stem.porter import PorterStemmer#词干提取\n",
    "from nltk.stem import WordNetLemmatizer#词形还原\n",
    "po_stem=PorterStemmer()\n",
    "word3=[po_stem.stem(w) for w in word2]\n",
    "w_lem=WordNetLemmatizer()\n",
    "word3=[w_lem.lemmatize(w) for w in word2]\n",
    "word3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看哪些词被提取词干和还原了\n",
    "t=[w for w in word2 if w not in word3]\n",
    "print(t)\n",
    "t=[w for w in word3 if w not in word2]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 现在把上面的所有步骤都整合在一起，写成一个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2wordbag(raw_txt):\n",
    "    l_stem=LancasterStemmer()#不能用PorterStemmer，对于一些未登录词会报错，比如OED\n",
    "    w_lem=WordNetLemmatizer()\n",
    "    txt=BeautifulSoup(raw_txt,'lxml')\n",
    "    txt=txt.get_text().lower()\n",
    "    txt=re.sub('[^a-zA-Z]',' ',txt)\n",
    "    word=word_tokenize(txt)\n",
    "    stops=set(stopwords.words('english'))\n",
    "    word=[w for w in word if w not in stops]\n",
    "    word=[l_stem.stem(w) for w in word]\n",
    "    word=[w_lem.lemmatize(w) for w in word]\n",
    "    return ' '.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 1000 of 25000\n",
      "process: 2000 of 25000\n",
      "process: 3000 of 25000\n",
      "process: 4000 of 25000\n",
      "process: 5000 of 25000\n",
      "process: 6000 of 25000\n",
      "process: 7000 of 25000\n",
      "process: 8000 of 25000\n",
      "process: 9000 of 25000\n",
      "process: 10000 of 25000\n",
      "process: 11000 of 25000\n",
      "process: 12000 of 25000\n",
      "process: 13000 of 25000\n",
      "process: 14000 of 25000\n",
      "process: 15000 of 25000\n",
      "process: 16000 of 25000\n",
      "process: 17000 of 25000\n",
      "process: 18000 of 25000\n",
      "process: 19000 of 25000\n",
      "process: 20000 of 25000\n",
      "process: 21000 of 25000\n",
      "process: 22000 of 25000\n",
      "process: 23000 of 25000\n",
      "process: 24000 of 25000\n",
      "process: 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "size=train_data['review'].size\n",
    "all_wordbag=[]\n",
    "for i in range(0,size):\n",
    "    if (i+1)%1000==0:\n",
    "        print(\"process: %d of %d\"%(i+1,size))\n",
    "    t=txt2wordbag(train_data['review'][i])\n",
    "    all_wordbag.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn提取词袋特征\n",
    "从评论中提取的词中选取频次最高的前5000个单词作为词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer(analyzer = \"word\",tokenizer = None,preprocessor = None, stop_words = None,max_features = 5000)\n",
    "train_data_feature=vect.fit_transform(all_wordbag)\n",
    "train_data_feature=train_data_feature.toarray()#得到的one-hot词向量\n",
    "train_data_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab',\n",
       " 'abandon',\n",
       " 'abbot',\n",
       " 'abc',\n",
       " 'abduc',\n",
       " 'abl',\n",
       " 'abomin',\n",
       " 'aborigin',\n",
       " 'abort',\n",
       " 'abound',\n",
       " 'abraham',\n",
       " 'abrupt',\n",
       " 'absolv',\n",
       " 'absorb',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'abud',\n",
       " 'abund',\n",
       " 'abus',\n",
       " 'abysm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()#词汇表\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林(、极端随机森林、梯度提升)预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "forest = forest.fit(train_data_feature,train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: 1000 of 25000\n",
      "process: 2000 of 25000\n",
      "process: 3000 of 25000\n",
      "process: 4000 of 25000\n",
      "process: 5000 of 25000\n",
      "process: 6000 of 25000\n",
      "process: 7000 of 25000\n",
      "process: 8000 of 25000\n",
      "process: 9000 of 25000\n",
      "process: 10000 of 25000\n",
      "process: 11000 of 25000\n",
      "process: 12000 of 25000\n",
      "process: 13000 of 25000\n",
      "process: 14000 of 25000\n",
      "process: 15000 of 25000\n",
      "process: 16000 of 25000\n",
      "process: 17000 of 25000\n",
      "process: 18000 of 25000\n",
      "process: 19000 of 25000\n",
      "process: 20000 of 25000\n",
      "process: 21000 of 25000\n",
      "process: 22000 of 25000\n",
      "process: 23000 of 25000\n",
      "process: 24000 of 25000\n",
      "process: 25000 of 25000\n"
     ]
    }
   ],
   "source": [
    "test_data=pd.read_csv('data/testData.tsv',header=0,sep='\\t',quoting=3)\n",
    "size=test_data['review'].size\n",
    "all_wordbag=[]\n",
    "for i in range(0,size):\n",
    "    if (i+1)%1000==0:\n",
    "        print(\"process: %d of %d\"%(i+1,size))\n",
    "    t=txt2wordbag(test_data['review'][i])\n",
    "    all_wordbag.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_feature=vect.fit_transform(all_wordbag)\n",
    "test_data_feature=test_data_feature.toarray()#得到的one-hot词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=forest.predict(test_data_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 创建提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame( data={\"id\":test_data[\"id\"], \"sentiment\":result} )\n",
    "\n",
    "output.to_csv( \"data/result/Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
